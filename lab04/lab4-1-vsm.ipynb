{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** K\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Xavier Jeanmonod\n",
    "* Adrian Baudat\n",
    "* Simon Wicky\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import math\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def remove_punct(description):\n",
    "    return description.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "def remove_XA0(description):\n",
    "    return description.replace(u'\\xa0',u' ')\n",
    "\n",
    "def split_words(description):\n",
    "    newString = description[0]\n",
    "    for i in range(1,len(description) - 1):\n",
    "        newString += description[i]\n",
    "        if description[i].islower() and description[i+1].isupper():\n",
    "            newString += ' '\n",
    "    newString += description[len(description)-1]\n",
    "    return newString\n",
    "\n",
    "def remove_words(wordList, to_remove):\n",
    "    return [word for word in wordList if word not in to_remove]\n",
    "\n",
    "def remove_number(wordlist):\n",
    "    return [word for word in wordlist if  not word.isdigit()]\n",
    "\n",
    "def extreme_words(rare,frequent):\n",
    "    list_all = []\n",
    "    for course in courses:\n",
    "        list_all += clean(course[\"description\"])\n",
    "    hist = list(Counter(list_all).items())\n",
    "    rare_words= [word for (word, occ) in hist if occ < rare]\n",
    "    frequent_words = [word for (word, occ) in hist if occ > frequent]\n",
    "    return rare_words, frequent_words\n",
    "\n",
    "def clean(text):\n",
    "    final = remove_XA0(text)\n",
    "    final = remove_punct(final)\n",
    "    final = split_words(final)\n",
    "    final_list = remove_words(final.lower().split(' '), stopwords)\n",
    "    final_list = remove_number(final_list)\n",
    "    return final_list\n",
    "\n",
    "\n",
    "rare_words, frequent_words = extreme_words(10,300)\n",
    "\n",
    "def full_clean(text):\n",
    "    final_list = clean(text)\n",
    "    final_list = remove_words(final_list,rare_words)\n",
    "    final_list = remove_words(final_list,frequent_words)\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing operations:\n",
    "* Remove the punctuation to avoid having words ending with \",\" or \".\"\n",
    "* Replace the \\xa0 character (special space) by a simple space.\n",
    "* Remove stopwords, because they're essentially meaningless\n",
    "* Remove rare words, because the TF-IDF will be biased for these.\n",
    "* Remove frequent words, words like \"method\" or \"student\" aren't really relevant\n",
    "* Artificial split of words if an capital letter has a lowercase letter before. Ex from MSE-440: \"materialsConstituentsProcessing\" will be split in 3 words. Probably dirty data\n",
    "* Remove number. Having \"20\" or \"30\" in the corpus is meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_courses = []\n",
    "all_courses_description = []\n",
    "for course in courses:\n",
    "    new_description = full_clean(course['description'])\n",
    "    all_courses_description.append(new_description)\n",
    "    cleaned_courses.append({'courseId': course['courseId'], 'name': course['name'],'description': new_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acquired', 'ad', 'ad', 'algebra', 'algebra', 'algorithms', 'algorithms', 'analytics', 'analytics', 'analyze', 'balance', 'based', 'based', 'cathedra', 'chains', 'class', 'class', 'class', 'clustering', 'clustering', 'collection', 'combination', 'communication', 'community', 'community', 'computing', 'computing', 'concrete', 'current', 'dedicated', 'designed', 'detection', 'detection', 'develop', 'draw', 'efficiency', 'explore', 'explore', 'explore', 'explore', 'fields', 'final', 'functions', 'fundamental', 'good', 'graph', 'graphs', 'handson', 'homeworks', 'homeworks', 'infrastructure', 'internet', 'internet', 'java', 'key', 'knowledge', 'lab', 'laboratory', 'labs', 'labs', 'largescale', 'largescale', 'largescale', 'linear', 'linear', 'machine', 'machine', 'main', 'markov', 'material', 'media', 'midterm', 'mining', 'mining', 'mining', 'modeling', 'networking', 'networking', 'networking', 'networks', 'number', 'number', 'online', 'online', 'online', 'online', 'online', 'past', 'practical', 'practice', 'provide', 'questions', 'real', 'realworld', 'realworld', 'realworld', 'realworld', 'reduction', 'related', 'retrieval', 'search', 'services', 'services', 'services', 'services', 'services', 'sessions', 'sessions', 'social', 'social', 'social', 'social', 'social', 'start', 'statistics', 'stochastic', 'stream', 'stream', 'structures', 'theoretical', 'topic', 'typical', 'user', 'weekly', 'world']\n"
     ]
    }
   ],
   "source": [
    "ix = cleaned_courses[43][\"description\"]\n",
    "ix.sort()\n",
    "print(ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(descriptionlist):\n",
    "    c = Counter(descriptionlist)\n",
    "    maxOcc = max(c.values())\n",
    "    return dict([(word,c[word]/maxOcc) for word in descriptionlist])\n",
    "def idf(descriptionlist):\n",
    "    N = len(all_courses_description)\n",
    "    out = {}\n",
    "    for word in descriptionlist:\n",
    "        counter = 0\n",
    "        for course_word in all_courses_description:\n",
    "            if word in course_word:\n",
    "                counter += 1\n",
    "        out[word] = - math.log(counter/N) / math.log(2)\n",
    "    return out\n",
    "def tf_idf(descriptionlist):\n",
    "    out = {}\n",
    "    tf_out = tf(descriptionlist)\n",
    "    idf_out = idf(descriptionlist)\n",
    "    for word in descriptionlist:\n",
    "        out[word] = tf_out[word] * idf_out[word]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set([word for course in all_courses_description for word in course]))\n",
    "#mapping from a word to an index\n",
    "word2index = dict([(all_words[i],i) for i in range(len(all_words))])\n",
    "#mappind from course id to an index\n",
    "course2index= dict([(courses[i][\"courseId\"],i) for i in range(len(courses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros(shape=(len(courses),len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "#Expensive calcuation\n",
    "for course in cleaned_courses:\n",
    "    if course2index[course[\"courseId\"]] % 100 == 0:\n",
    "        print(course2index[course[\"courseId\"]])\n",
    "    tf_idf_dict = tf_idf(course[\"description\"])\n",
    "    for word in tf_idf_dict.keys():\n",
    "        matrix[course2index[course[\"courseId\"]]][word2index[word]] = tf_idf_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'matrix' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result of IX course :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('services', 5.650629418370151),\n",
       " ('realworld', 4.922503807119468),\n",
       " ('online', 4.8801112644929185),\n",
       " ('social', 4.608809242675524),\n",
       " ('mining', 4.042855355772294),\n",
       " ('explore', 3.864961331209578),\n",
       " ('networking', 3.767196384589916),\n",
       " ('largescale', 3.4987209984071828),\n",
       " ('internet', 2.7722949350251547),\n",
       " ('stream', 2.7722949350251547),\n",
       " ('ad', 2.6272669032712717),\n",
       " ('community', 2.6272669032712717),\n",
       " ('clustering', 2.511464256393278),\n",
       " ('analytics', 2.461251903559734),\n",
       " ('labs', 2.2272669032712713)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_ix = list(tf_idf(ix).items())\n",
    "tf_idf_ix.sort(key = lambda x: -x[1])\n",
    "print(\"Top result of IX course :\")\n",
    "tf_idf_ix[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the TF factor is normalized by the maximum occurence of a word in this document, a high TF-IDF score means that these words are specific to this document, that they are not present in many other documents. The small score is either due to low presence of the word in the document, or the high presence in the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(doc1, doc2):\n",
    "    num = np.dot(doc1,doc2)\n",
    "    denom = np.linalg.norm(doc1) * np.linalg.norm(doc2)\n",
    "    return num / denom\n",
    "\n",
    "def sim_list(doc):\n",
    "    result = []\n",
    "    for i in range(len(matrix)):\n",
    "        value = sim(doc,matrix[i])\n",
    "        if value > 0:\n",
    "            for courseId, index in course2index.items():\n",
    "                if index == i:\n",
    "                    result.append((courseId,courses[index][\"name\"],value))\n",
    "    result.sort(key=lambda x: -x[2])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MGT-484', 'Applied probability & stochastic processes', 0.622617931178696), ('MATH-332', 'Applied stochastic processes', 0.5341955705371184), ('EE-605', 'Statistical Sequence Processing', 0.4423097584999178), ('COM-516', 'Markov chains and algorithmic applications', 0.38026569817022693), ('EE-516', 'Data analysis and model classification', 0.14350135980025172)]\n"
     ]
    }
   ],
   "source": [
    "markov_chain_vector = np.zeros(shape=len(all_words))\n",
    "markov_chain_vector[word2index[\"markov\"]] = 1\n",
    "markov_chain_top5 = sim_list(markov_chain_vector)[:5]\n",
    "print(markov_chain_top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four course seems pretty relevant, since they all deal with probability or stochastic process.\n",
    "The fifth one is arguable, we can see that the similarity score is significantly lower than the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important note\n",
    "\"Facebook\" is used only once in the whole corpus, hence was removed during preprocessing. \n",
    "For the need of this lab, we preprocessed without removing words, but the matrix construction takes too long.\n",
    "We were still able to extract a result, but the following code won't work, because facebook is no longer a word in the corpus\n",
    "##### Output we got for facebook :\n",
    "[('EE-727', 'Computational Social Media', 0.17572585110979233)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facebook_vector = np.zeros(shape=len(all_words))\n",
    "#facebook_vector[word2index[\"facebook\"]] = 1\n",
    "#facebook_top = sim_list(facebook_vector)\n",
    "#print(facebook_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the note before, \"Facebook\" only occurs once in one course descriptiob. This is why we only get this result"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
